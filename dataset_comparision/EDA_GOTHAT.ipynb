{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahm/anaconda3/envs/trans_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "# import networkx as nx\n",
    "from datetime import datetime\n",
    "import time\n",
    "import preprocessor as p\n",
    "import ast\n",
    "from wordsegment import load, segment\n",
    "import random\n",
    "load()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy, shapiro, mannwhitneyu\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from spacy.lang.hi import STOP_WORDS as STOP_WORDS_HI\n",
    "from spacy.lang.de import STOP_WORDS as STOP_WORDS_DE\n",
    "from spacy.lang.es import STOP_WORDS as STOP_WORDS_ES\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords2 = ['तुम','मेरी','मुझे','क्योंकि','हम','प्रति','अबकी','दे',\n",
    "                              'आगे','', 'अर्थात', 'कुछ', 'तेरी', 'साबुत', 'अपनि', 'हूं',\n",
    "                              'काफि', 'यिह', 'जा' ,'दे', 'देकर' ,'रह', 'कह' , 'कर' ,\n",
    "                              'कहा', 'बात' , 'जिन्हों', 'किर', 'कोई','माननीय','शहर','बताएं',\n",
    "                              'कौनसी','क्लिक','किसकी','बड़े','मैं','and','रही','आज','लें',\n",
    "                              'आपके','मिलकर','सब','मेरे','जी','श्री','वैसा','आपका','अंदर',\n",
    "                              'अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'आदि', 'आप', 'इत्यादि',\n",
    "                              'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों', 'इस', 'इसका', 'इसकी', \n",
    "                              'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके',\n",
    "                              'उनको', 'उन्हीं', 'उन्हें', 'उन्हों', 'उस', 'उसके', 'उसी', 'उसे',\n",
    "                              'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर','करता', 'करते',\n",
    "                              'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि',\n",
    "                              'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस', 'किसी',\n",
    "                              'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन',\n",
    "                              'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जा', 'जितना', 'जिन',\n",
    "                              'जिन्हें', 'जिन्हों', 'जिस', 'जिसे', 'जीधर', 'जैसा', 'जैसे', 'जो',\n",
    "                              'तक', 'तब', 'तरह', 'तिन', 'तिन्हें', 'तिन्हों', 'तिस',\n",
    "                              'तिसे', 'तो', 'था', 'थी', 'थे', 'दबारा', 'दिया', 'दुसरा', \n",
    "                              'दूसरे', 'दो', 'द्वारा', 'न', 'नहीं', 'ना', 'निहायत', 'नीचे',\n",
    "                              'ने', 'पर', 'पर', 'पहले', 'पूरा', 'पे', 'फिर', 'बनी', 'बही',\n",
    "                              'बहुत', 'बाद', 'बाला', 'बिलकुल', 'भी', 'भीतर', 'मगर', 'मानो',\n",
    "                              'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें',\n",
    "                              'रहा', 'रहे', 'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वर्ग', 'वह',\n",
    "                              'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'वग़ैरह', 'संग', 'सकता',\n",
    "                              'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो',\n",
    "                              'ही', 'हुआ', 'हुई', 'हुए', 'है', 'हैं', 'हो', 'होता', 'होती', 'होते',\n",
    "                              'होना', 'होने', 'अपनि', 'जेसे', 'होति', 'सभि', 'तिंहों', 'इंहों', 'दवारा',\n",
    "                              'इसि', 'किंहें', 'थि', 'उंहों', 'ओर', 'जिंहें', 'वहिं', 'अभि', 'बनि', 'हि',\n",
    "                              'उंहिं', 'उंहें', 'हें', 'वगेरह', 'एसे', 'रवासा', 'कोन', 'निचे', 'काफि',\n",
    "                              'उसि', 'पुरा', 'भितर', 'हे', 'बहि', 'वहां', 'कोइ', 'यहां', 'जिंहों', \n",
    "                              'तिंहें', 'किसि', 'कइ', 'यहि', 'इंहिं', 'जिधर', 'इंहें', 'अदि', 'इतयादि',\n",
    "                              'हुइ', 'कोनसा', 'इसकि', 'दुसरे', 'जहां', 'अप', 'किंहों', 'उनकि', 'भि', \n",
    "                              'वरग', 'हुअ', 'जेसा', 'नहिं']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_LIST = set(stopwords.words('english'))\n",
    "with open('/home/sarahm/LCS2-Hate-Speech-Detection-Diffusion/mallet_stoplist.txt',\"r\") as f:         \n",
    "        MALLET_STOPLIST = f.readlines()\n",
    "MALLET_STOPLIST = [x.replace(\"\\n\", \"\") for x in MALLET_STOPLIST] \n",
    "STOP_LIST = STOP_LIST.union(set(MALLET_STOPLIST))\n",
    "STOP_LIST = list(STOP_LIST)\n",
    "STOP_LIST.extend(STOP_WORDS_HI)\n",
    "STOP_LIST.extend(STOP_WORDS_DE)\n",
    "STOP_LIST.extend(STOP_WORDS_ES)\n",
    "STOP_LIST.extend([\"rt\",\":\",\"…\",\"&amp\",\"\",\"https://t.co/…\"])\n",
    "STOP_LIST += stopwords2\n",
    "stemmer = PorterStemmer()\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "\n",
    "def preprocess(text_string):\n",
    "    text_string = text_string.strip().lower()\n",
    "    space_pattern = '\\s+'\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(mention_regex, ' ', parsed_text)\n",
    "    text_string = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', parsed_text, flags=re.MULTILINE) \n",
    "    parsed_text = parsed_text.translate(table)\n",
    "    parsed_text = parsed_text.strip()\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tokens = []\n",
    "    for word in tweet.split():\n",
    "        if word in STOP_LIST:\n",
    "            continue\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset,prune=False,high_prune=500,low_prune=750): \n",
    "    words = [] \n",
    "    for data in dataset : \n",
    "        tokens = tokenize(preprocess(data))\n",
    "        words.extend(tokens)\n",
    "    if prune:\n",
    "        mc = Counter(words).most_common()\n",
    "        vocab_prune=[]\n",
    "        for word,count in mc[:high_prune]:\n",
    "            vocab_prune.extend([word]*count)\n",
    "        for word,count in mc[-low_prune:]:\n",
    "            vocab_prune.extend([word]*count)\n",
    "        return vocab_prune\n",
    "    else:\n",
    "        return words\n",
    "    \n",
    "def get_combined_df(base):\n",
    "    INPUT_FOLDER = \"/home/sarahm/LCS2-Hate-Speech-Detection-Diffusion/dataset/ORG_DATASETS/\"\n",
    "    train_df = pd.read_csv(INPUT_FOLDER+base+\"/train_final.csv\",lineterminator=\"\\n\")\n",
    "    test_df = pd.read_csv(INPUT_FOLDER+base+\"/test_final.csv\",lineterminator=\"\\n\")\n",
    "    val_df = pd.read_csv(INPUT_FOLDER+base+\"/val_final.csv\",lineterminator=\"\\n\")\n",
    "    df_combined = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
    "    df_combined = df_combined.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    print(df_combined.shape,train_df.shape,val_df.shape,test_df.shape)\n",
    "    return df_combined, train_df, val_df, test_df\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "    \n",
    "def for_4_classes(BASE,prune=True):\n",
    "    df, train_df, val_df, test_df = get_combined_df(BASE)\n",
    "    \n",
    "    column = 'clean_text'\n",
    "#     if BASE==\"HOPN\":\n",
    "#         column=\"text\"\n",
    "        \n",
    "    hate_dataset = preprocess_dataset(df[df['label'] == 0][column].values.tolist(),prune=prune)\n",
    "    print(\"hate_dataset size: \", len(hate_dataset))\n",
    "    offensive_dataset = preprocess_dataset(df[df['label'] == 1][column].values.tolist(),prune=prune)\n",
    "    print(\"offensive_dataset size: \", len(offensive_dataset))\n",
    "    provocative_dataset = preprocess_dataset(df[df['label'] == 2][column].values.tolist(),prune=prune)\n",
    "    print(\"provocative_dataset size: \", len(provocative_dataset))\n",
    "    control_dataset = preprocess_dataset(df[df['label'] == 3][column].values.tolist(),prune=prune)\n",
    "    print(\"control_dataset size: \", len(control_dataset))\n",
    "\n",
    "    hate_set = set(hate_dataset)\n",
    "    print(\"hate_set size: \", len(hate_set))\n",
    "    offensive_set = set(offensive_dataset)\n",
    "    print(\"offensive_set size: \", len(offensive_set))\n",
    "    provocative_set = set(provocative_dataset)\n",
    "    print(\"provocative_set size: \", len(provocative_set))\n",
    "    control_set = set(control_dataset)\n",
    "    print(\"control_set size: \", len(control_set))\n",
    "    union_set = control_set.union(provocative_set.union(offensive_set.union(hate_set)))\n",
    "    print(\"union_set size: \", len(union_set))\n",
    "\n",
    "    hate_counter = Counter(hate_dataset)\n",
    "    offensive_counter = Counter(offensive_dataset)\n",
    "    provocative_counter = Counter(provocative_dataset)\n",
    "    control_counter = Counter(control_dataset)\n",
    "\n",
    "    pdf_hate = []\n",
    "    pdf_offensive = []\n",
    "    pdf_provocative = []\n",
    "    pdf_control = []\n",
    "    pdf_hate_raw = []\n",
    "    pdf_offensive_raw = []\n",
    "    pdf_provocative_raw = []\n",
    "    pdf_control_raw = []\n",
    "\n",
    "    N = len(hate_dataset) + len(offensive_dataset) + len(provocative_dataset) + len(control_dataset)\n",
    "    V = len(union_set)\n",
    "\n",
    "    for word in union_set : \n",
    "        if word in hate_set : \n",
    "            pdf_hate.append((hate_counter[word]+1)/(len(hate_dataset) + len(hate_set)))\n",
    "        else : \n",
    "            pdf_hate.append(1/(N+V))\n",
    "        pdf_hate_raw.append(hate_counter.get(word,0))\n",
    "        if word in offensive_set : \n",
    "            pdf_offensive.append((offensive_counter[word]+1)/(len(offensive_dataset) + len(offensive_set)))\n",
    "        else : \n",
    "            pdf_offensive.append(1/(N+V))\n",
    "        pdf_offensive_raw.append(offensive_counter.get(word,0))\n",
    "        if word  in provocative_set : \n",
    "            pdf_provocative.append((provocative_counter[word]+1)/(len(provocative_dataset) + len(provocative_set)))\n",
    "        else : \n",
    "            pdf_provocative.append(1/(N+V))\n",
    "        pdf_provocative_raw.append(provocative_counter.get(word,0))    \n",
    "        if word in control_set : \n",
    "            pdf_control.append((control_counter[word]+1)/(len(control_dataset) + len(control_set)))\n",
    "        else : \n",
    "            pdf_control.append(1/(N+V))\n",
    "        pdf_control_raw.append(control_counter.get(word,0))  \n",
    "    print(sum(pdf_hate))\n",
    "    print(sum(pdf_offensive))\n",
    "    print(sum(pdf_provocative))\n",
    "    print(sum(pdf_control))\n",
    "    \n",
    "    print(\"\\n\\n ---- JS\")\n",
    "    print(\"Jenson Shannon Distance for H O\", JSD(pdf_hate, pdf_offensive))\n",
    "    print(\"Jenson Shannon Distance for H P\", JSD(pdf_hate, pdf_provocative))\n",
    "    print(\"Jenson Shannon Distance for H N\", JSD(pdf_hate, pdf_control))\n",
    "    print(\"Jenson Shannon Distance for O P\", JSD(pdf_offensive, pdf_provocative))\n",
    "    print(\"Jenson Shannon Distance for O N\", JSD(pdf_offensive, pdf_control))\n",
    "    print(\"Jenson Shannon Distance for P N\", JSD(pdf_provocative, pdf_control))\n",
    "    \n",
    "#     print(\"\\n\\n ---- MW\")\n",
    "#     print(\"2 way MW TEST for H O:: \", mannwhitneyu(pdf_hate_raw, pdf_offensive_raw))\n",
    "#     print(\"2 way MW TEST  for H P:: \", mannwhitneyu(pdf_hate_raw, pdf_provocative_raw))\n",
    "#     print(\"2 way MW TEST  for H N:: \", mannwhitneyu(pdf_hate_raw, pdf_control_raw))\n",
    "#     print(\"2 way MW TEST  for O P:: \", mannwhitneyu(pdf_offensive_raw, pdf_provocative_raw))\n",
    "#     print(\"2 way MW TEST  for O N:: \", mannwhitneyu(pdf_offensive_raw, pdf_control_raw))\n",
    "#     print(\"2 way MW TEST for P N:: \", mannwhitneyu(pdf_provocative_raw, pdf_control_raw))\n",
    "    \n",
    "def for_3_classes(BASE,prune=True):\n",
    "    df, train_df, val_df, test_df = get_combined_df(BASE)\n",
    "    column = 'clean_text'\n",
    "\n",
    "    hate_dataset = preprocess_dataset(df[df['label'] == 0][column].values.tolist(),prune=prune)\n",
    "    print(\"hate_dataset size: \", len(hate_dataset))\n",
    "    offensive_dataset = preprocess_dataset(df[df['label'] == 1][column].values.tolist(),prune=prune)\n",
    "    print(\"offensive_dataset size: \", len(offensive_dataset))\n",
    "    control_dataset = preprocess_dataset(df[df['label'] == 2][column].values.tolist(),prune=prune)\n",
    "    print(\"control_dataset size: \", len(control_dataset))\n",
    "\n",
    "    hate_set = set(hate_dataset)\n",
    "    print(\"hate_set size: \", len(hate_set))\n",
    "    offensive_set = set(offensive_dataset)\n",
    "    control_set = set(control_dataset)\n",
    "    print(\"control_set size: \", len(control_set))\n",
    "    union_set = control_set.union((offensive_set.union(hate_set)))\n",
    "    print(\"union_set size: \", len(union_set))\n",
    "\n",
    "    hate_counter = Counter(hate_dataset)\n",
    "    offensive_counter = Counter(offensive_dataset)\n",
    "    control_counter = Counter(control_dataset)\n",
    "\n",
    "    pdf_hate = []\n",
    "    pdf_offensive = []\n",
    "    pdf_control = []\n",
    "\n",
    "    pdf_hate_raw = []\n",
    "    pdf_offensive_raw=[]\n",
    "    pdf_control_raw = []\n",
    "    N = len(hate_dataset) + len(offensive_dataset) + len(control_dataset)\n",
    "    V = len(union_set)\n",
    "\n",
    "    for word in union_set : \n",
    "        if word in hate_set : \n",
    "            pdf_hate.append((hate_counter[word]+1)/(len(hate_dataset) + len(hate_set)))\n",
    "        else : \n",
    "            pdf_hate.append(1/(N+V))\n",
    "        pdf_hate_raw.append(hate_counter.get(word,0))\n",
    "        if word in offensive_set : \n",
    "            pdf_offensive.append((offensive_counter[word]+1)/(len(offensive_dataset) + len(offensive_set)))\n",
    "        else : \n",
    "            pdf_offensive.append(1/(N+V))\n",
    "        pdf_offensive_raw.append(offensive_counter.get(word,0))\n",
    "        if word in control_set : \n",
    "            pdf_control.append((control_counter[word]+1)/(len(control_dataset) + len(control_set)))\n",
    "        else : \n",
    "            pdf_control.append(1/(N+V))\n",
    "        pdf_control_raw.append(control_counter.get(word,0)) \n",
    "\n",
    "    print(sum(pdf_hate))\n",
    "    print(sum(pdf_offensive))\n",
    "    print(sum(pdf_control))\n",
    "    \n",
    "    print(\"\\n\\n ---- JS\")\n",
    "    print(\"Jenson Shannon Distance for H O\", JSD(pdf_hate, pdf_offensive))\n",
    "    print(\"Jenson Shannon Distance for H N\", JSD(pdf_hate, pdf_control))\n",
    "    print(\"Jenson Shannon Distance for O N\", JSD(pdf_offensive, pdf_control))\n",
    "#     print(\"\\n\\n ---- MW\")\n",
    "#     print(\"2 way MW TEST  for H O:: \", mannwhitneyu(pdf_hate_raw, pdf_offensive_raw))\n",
    "#     print(\"2 way MW TEST  for H N:: \", mannwhitneyu(pdf_hate_raw, pdf_control_raw))\n",
    "#     print(\"2 way MW TEST  for O N:: \", mannwhitneyu(pdf_offensive_raw, pdf_control_raw))\n",
    "    \n",
    "def for_other_datasets(BASE,prune=True):\n",
    "    df, train_df, val_df, test_df = get_combined_df(BASE)\n",
    "    column = 'clean_text'\n",
    "\n",
    "    hate_dataset = preprocess_dataset(df[df['label'] == 0][column].values.tolist(),prune=prune)\n",
    "    print(\"hate_dataset size: \", len(hate_dataset))\n",
    "    control_dataset = preprocess_dataset(df[df['label'] == 1][column].values.tolist(),prune=prune)\n",
    "    print(\"control_dataset size: \", len(control_dataset))\n",
    "\n",
    "    hate_set = set(hate_dataset)\n",
    "    print(\"hate_set size: \", len(hate_set))\n",
    "    control_set = set(control_dataset)\n",
    "    print(\"control_set size: \", len(control_set))\n",
    "    union_set = control_set.union(hate_set)\n",
    "    print(\"union_set size: \", len(union_set))\n",
    "\n",
    "    hate_counter = Counter(hate_dataset)\n",
    "    control_counter = Counter(control_dataset)\n",
    "\n",
    "    pdf_hate = []\n",
    "    pdf_control = []\n",
    "    pdf_hate_raw = []\n",
    "    pdf_control_raw = []\n",
    "    N = len(hate_dataset) + len(control_dataset)\n",
    "    V = len(union_set)\n",
    "\n",
    "    for word in union_set : \n",
    "        if word in hate_set : \n",
    "            pdf_hate.append((hate_counter[word]+1)/(len(hate_dataset) + len(hate_set)))\n",
    "        else : \n",
    "            pdf_hate.append(1/(N+V))\n",
    "        pdf_hate_raw.append(hate_counter.get(word,0))\n",
    "        if word in control_set : \n",
    "            pdf_control.append((control_counter[word]+1)/(len(control_dataset) + len(control_set)))\n",
    "        else : \n",
    "            pdf_control.append(1/(N+V))\n",
    "        pdf_control_raw.append(control_counter.get(word,0)) \n",
    "    print(sum(pdf_hate))\n",
    "    print(sum(pdf_control))\n",
    "\n",
    "    print(\"\\n\\n ---- JS\")\n",
    "    print(\"Jenson Shannon Distance for H N\", JSD(pdf_hate, pdf_control))\n",
    "#     print(\"\\n\\n ---- MW\")\n",
    "#     print(\"2 way MW TEST  for H N:: \", mannwhitneyu(pdf_hate_raw, pdf_control_raw))\n",
    "\n",
    "\n",
    "def binary_compare(BASE,multi=True,prune=True):\n",
    "    df, train_df, val_df, test_df = get_combined_df(BASE)\n",
    "    column = 'clean_text'\n",
    "#     if BASE==\"HOPN\":\n",
    "#         column=\"text\"\n",
    "    num_unq = len(df[\"label\"].unique())\n",
    "    if num_unq >2 and multi:\n",
    "        print(\"Here\")\n",
    "        hate_dataset = preprocess_dataset(df[df['label'] <=1][column].values.tolist(),prune=prune)\n",
    "        print(\"hate_dataset size: \", len(hate_dataset))\n",
    "        control_dataset = preprocess_dataset(df[df['label'] > 1][column].values.tolist(),prune=prune)\n",
    "        print(\"control_dataset size: \", len(control_dataset))\n",
    "    else:\n",
    "        hate_dataset = preprocess_dataset(df[df['label'] == 0][column].values.tolist(),prune=prune)\n",
    "        print(\"hate_dataset size: \", len(hate_dataset))\n",
    "        control_dataset = preprocess_dataset(df[df['label'] == 1][column].values.tolist(),prune=prune)\n",
    "        print(\"control_dataset size: \", len(control_dataset))\n",
    "    hate_set = set(hate_dataset)\n",
    "    print(\"hate_set size: \", len(hate_set))\n",
    "    control_set = set(control_dataset)\n",
    "    print(\"control_set size: \", len(control_set))\n",
    "    union_set = control_set.union(hate_set)\n",
    "    print(\"union_set size: \", len(union_set))\n",
    "\n",
    "    hate_counter = Counter(hate_dataset)\n",
    "    control_counter = Counter(control_dataset)\n",
    "\n",
    "    pdf_hate = []\n",
    "    pdf_control = []\n",
    "    pdf_hate_raw = []\n",
    "    pdf_control_raw = []\n",
    "    N = len(hate_dataset) + len(control_dataset)\n",
    "    V = len(union_set)\n",
    "\n",
    "    for word in union_set : \n",
    "        if word in hate_set : \n",
    "            pdf_hate.append((hate_counter[word]+1)/(len(hate_dataset) + len(hate_set)))\n",
    "        else : \n",
    "            pdf_hate.append(1/(N+V))\n",
    "        pdf_hate_raw.append(hate_counter.get(word,0))\n",
    "        if word in control_set : \n",
    "            pdf_control.append((control_counter[word]+1)/(len(control_dataset) + len(control_set)))\n",
    "        else : \n",
    "            pdf_control.append(1/(N+V))\n",
    "        pdf_control_raw.append(control_counter.get(word,0)) \n",
    "    print(sum(pdf_hate))\n",
    "    print(sum(pdf_control))\n",
    "\n",
    "    print(\"\\n\\n ---- JS\")\n",
    "    print(\"Jenson Shannon Distance for H N\", JSD(pdf_hate, pdf_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51367, 20) (41093, 20) (5137, 20) (5137, 20)\n",
      "hate_dataset size:  45999\n",
      "offensive_dataset size:  88423\n",
      "provocative_dataset size:  110227\n",
      "control_dataset size:  260302\n",
      "hate_set size:  1250\n",
      "offensive_set size:  1250\n",
      "provocative_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  3799\n",
      "1.005010319410342\n",
      "1.0050103194103392\n",
      "1.0050103194103484\n",
      "1.0050103194103286\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H O 0.1354406709671649\n",
      "Jenson Shannon Distance for H P 0.0872160407967375\n",
      "Jenson Shannon Distance for H N 0.11819917708809587\n",
      "Jenson Shannon Distance for O P 0.11858631855534867\n",
      "Jenson Shannon Distance for O N 0.12101396453080641\n",
      "Jenson Shannon Distance for P N 0.06379174493749891\n"
     ]
    }
   ],
   "source": [
    "for_4_classes(BASE=\"HOPN\",prune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59189, 4) (37880, 4) (9471, 4) (11838, 4)\n",
      "hate_dataset size:  9982\n",
      "offensive_dataset size:  40077\n",
      "provocative_dataset size:  26106\n",
      "control_dataset size:  105562\n",
      "hate_set size:  1250\n",
      "offensive_set size:  1250\n",
      "provocative_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  4059\n",
      "1.0151195461445035\n",
      "1.0151195461445022\n",
      "1.0151195461445306\n",
      "1.0151195461444988\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H O 0.20482237651287338\n",
      "Jenson Shannon Distance for H P 0.46707207975029263\n",
      "Jenson Shannon Distance for H N 0.3187275856004806\n",
      "Jenson Shannon Distance for O P 0.46786129698656664\n",
      "Jenson Shannon Distance for O N 0.31811113178723754\n",
      "Jenson Shannon Distance for P N 0.2431742246454875\n"
     ]
    }
   ],
   "source": [
    "for_4_classes(BASE=\"Founta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 4) (15860, 4) (3966, 4) (4957, 4)\n",
      "hate_dataset size:  6449\n",
      "offensive_dataset size:  72641\n",
      "control_dataset size:  13522\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  3058\n",
      "1.0188982962265793\n",
      "1.0188982962266055\n",
      "1.0188982962266018\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H O 0.20446479111199295\n",
      "Jenson Shannon Distance for H N 0.34693070497570677\n",
      "Jenson Shannon Distance for O N 0.3320126371438755\n"
     ]
    }
   ],
   "source": [
    "for_3_classes(BASE=\"davidson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17657, 4) (11468, 4) (2868, 4) (3321, 4)\n",
      "hate_dataset size:  31733\n",
      "control_dataset size:  36899\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2227\n",
      "1.0137879450740026\n",
      "1.0137879450740164\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.24638369980651817\n"
     ]
    }
   ],
   "source": [
    "for_other_datasets(BASE=\"HASOC19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14100, 4) (10592, 4) (2648, 4) (860, 4)\n",
      "hate_dataset size:  21781\n",
      "control_dataset size:  35673\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2123\n",
      "1.0146533058059275\n",
      "1.014653305805919\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.11546945139757275\n"
     ]
    }
   ],
   "source": [
    "for_other_datasets(BASE=\"OLID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19600, 4) (13500, 4) (1500, 4) (4600, 4)\n",
      "hate_dataset size:  36475\n",
      "control_dataset size:  40672\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2140\n",
      "1.011225043197495\n",
      "1.0112250431974938\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.13068339166855147\n"
     ]
    }
   ],
   "source": [
    "for_other_datasets(BASE=\"HateEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21480, 4) (13747, 4) (3437, 4) (4296, 4)\n",
      "hate_dataset size:  34776\n",
      "control_dataset size:  47260\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2141\n",
      "1.010584839089053\n",
      "1.0105848390890793\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.12225002324481829\n"
     ]
    }
   ],
   "source": [
    "for_other_datasets(BASE=\"ImpHate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23748, 4) (15198, 4) (3800, 4) (4750, 4)\n",
      "hate_dataset size:  8260\n",
      "control_dataset size:  113607\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2223\n",
      "1.0078410830848197\n",
      "1.0078410830848454\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.2546480162061443\n"
     ]
    }
   ],
   "source": [
    "for_other_datasets(BASE=\"ICDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51367, 20) (41093, 20) (5137, 20) (5137, 20)\n",
      "Here\n",
      "hate_dataset size:  131175\n",
      "control_dataset size:  367111\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2137\n",
      "1.001772500464638\n",
      "1.0017725004646392\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.09107749661149464\n",
      "(59189, 4) (37880, 4) (9471, 4) (11838, 4)\n",
      "Here\n",
      "hate_dataset size:  48356\n",
      "control_dataset size:  126947\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2244\n",
      "1.0055985175756739\n",
      "1.0055985175756452\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.3245336912919928\n",
      "(24783, 4) (15860, 4) (3966, 4) (4957, 4)\n",
      "Here\n",
      "hate_dataset size:  77729\n",
      "control_dataset size:  13522\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2221\n",
      "1.010388137624103\n",
      "1.0103881376241195\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.3302780655141713\n",
      "(17657, 4) (11468, 4) (2868, 4) (3321, 4)\n",
      "hate_dataset size:  31733\n",
      "control_dataset size:  36899\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2227\n",
      "1.0137879450740026\n",
      "1.0137879450740164\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.24638369980651817\n",
      "(14100, 4) (10592, 4) (2648, 4) (860, 4)\n",
      "hate_dataset size:  21781\n",
      "control_dataset size:  35673\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2123\n",
      "1.0146533058059275\n",
      "1.014653305805919\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.11546945139757275\n",
      "(19600, 4) (13500, 4) (1500, 4) (4600, 4)\n",
      "hate_dataset size:  36475\n",
      "control_dataset size:  40672\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2140\n",
      "1.011225043197495\n",
      "1.0112250431974938\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.13068339166855147\n",
      "(21480, 4) (13747, 4) (3437, 4) (4296, 4)\n",
      "hate_dataset size:  34219\n",
      "control_dataset size:  46497\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2140\n",
      "1.010741527469342\n",
      "1.0107415274693397\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.12244299605977038\n",
      "(23748, 4) (15198, 4) (3800, 4) (4750, 4)\n",
      "hate_dataset size:  8260\n",
      "control_dataset size:  113607\n",
      "hate_set size:  1250\n",
      "control_set size:  1250\n",
      "union_set size:  2223\n",
      "1.0078410830848197\n",
      "1.0078410830848454\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for H N 0.2546480162061443\n"
     ]
    }
   ],
   "source": [
    "binary_compare(\"HOPN\")\n",
    "binary_compare(\"Founta\")\n",
    "binary_compare(\"davidson\")\n",
    "binary_compare(\"HASOC19\")\n",
    "binary_compare(\"OLID\")\n",
    "binary_compare(\"HateEval\")\n",
    "binary_compare(\"ImpHate\")\n",
    "binary_compare(\"ICDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_intra_dataset(BASE,prune=True):\n",
    "    df, train_df, val_df, test_df = get_combined_df(BASE)\n",
    "    \n",
    "    column = 'clean_text'\n",
    "        \n",
    "    train_dataset = preprocess_dataset(train_df[column].values.tolist(),prune=prune,high_prune=1000,low_prune=1000)\n",
    "    print(\"train_dataset size: \", len(train_dataset))\n",
    "    val_dataset = preprocess_dataset(val_df[column].values.tolist(),prune=prune,high_prune=1000,low_prune=1000)\n",
    "    print(\"val_dataset size: \", len(val_dataset))\n",
    "    test_dataset = preprocess_dataset(test_df[column].values.tolist(),prune=prune,high_prune=1000,low_prune=1000)\n",
    "    print(\"test_dataset size: \", len(test_dataset))\n",
    "\n",
    "    train_set = set(train_dataset)\n",
    "    print(\"train_set size: \", len(train_set))\n",
    "    val_set = set(val_dataset)\n",
    "    print(\"val_set size: \", len(val_set))\n",
    "    test_set = set(test_dataset)\n",
    "    print(\"test_set size: \", len(test_set))\n",
    "    \n",
    "    union_set = test_set.union(val_set.union(train_set))\n",
    "    print(\"union_set size: \", len(union_set))\n",
    "\n",
    "    train_counter = Counter(train_dataset)\n",
    "    val_counter = Counter(val_dataset)\n",
    "    test_counter = Counter(test_dataset)\n",
    "    \n",
    "    pdf_train = []\n",
    "    pdf_test = []\n",
    "    pdf_val = []\n",
    "    \n",
    "    pdf_train_raw = []\n",
    "    pdf_test_raw = []\n",
    "    pdf_val_raw = []\n",
    "\n",
    "    N = len(train_dataset) + len(test_dataset) + len(val_dataset)\n",
    "    V = len(union_set)\n",
    "\n",
    "    for word in union_set : \n",
    "        if word in train_set : \n",
    "            pdf_train.append((train_counter[word]+1)/(len(train_dataset) + len(train_set)))\n",
    "        else : \n",
    "            pdf_train.append(1/(N+V))\n",
    "        pdf_train_raw.append(train_counter.get(word,0))\n",
    "        if word in test_set : \n",
    "            pdf_test.append((test_counter[word]+1)/(len(test_dataset) + len(test_set)))\n",
    "        else : \n",
    "            pdf_test.append(1/(N+V))\n",
    "        pdf_test_raw.append(test_counter.get(word,0))\n",
    "        if word in val_set : \n",
    "            pdf_val.append((val_counter[word]+1)/(len(val_dataset) + len(val_set)))\n",
    "        else : \n",
    "            pdf_val.append(1/(N+V))\n",
    "        pdf_val_raw.append(val_counter.get(word,0))\n",
    "    print(sum(pdf_train))\n",
    "    print(sum(pdf_val))\n",
    "    print(sum(pdf_test))\n",
    "    \n",
    "    print(\"\\n\\n ---- JS\")\n",
    "    print(\"Jenson Shannon Distance for Train Val\", JSD(pdf_train, pdf_val))\n",
    "    print(\"Jenson Shannon Distance for Train Test\", JSD(pdf_train, pdf_test))\n",
    "    print(\"Jenson Shannon Distance for Val Test\", JSD(pdf_val, pdf_test))\n",
    "    \n",
    "#     print(\"\\n\\n ---- MW\")\n",
    "#     print(\"2 way MW TEST for Train Val:: \", mannwhitneyu(pdf_train_raw, pdf_val_raw))\n",
    "#     print(\"2 way MW TEST  for Train Tes:: \", mannwhitneyu(pdf_train_raw, pdf_test_raw))\n",
    "#     print(\"2 way MW TEST  Val Test:: \", mannwhitneyu(pdf_val_raw, pdf_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51367, 20) (41093, 20) (5137, 20) (5137, 20)\n",
      "train_dataset size:  456613\n",
      "val_dataset size:  58425\n",
      "test_dataset size:  58140\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4210\n",
      "1.003827582145793\n",
      "1.0038275821458011\n",
      "1.0038275821458271\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.03308766911812824\n",
      "Jenson Shannon Distance for Train Test 0.032659581917138414\n",
      "Jenson Shannon Distance for Val Test 0.05181550453670594\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"HOPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59189, 4) (37880, 4) (9471, 4) (11838, 4)\n",
      "train_dataset size:  140293\n",
      "val_dataset size:  36056\n",
      "test_dataset size:  44821\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4238\n",
      "1.0099286626916726\n",
      "1.0099286626916772\n",
      "1.0099286626916932\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.053788437832689315\n",
      "Jenson Shannon Distance for Train Test 0.04846997819102427\n",
      "Jenson Shannon Distance for Val Test 0.0754203522964162\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"Founta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 4) (15860, 4) (3966, 4) (4957, 4)\n",
      "train_dataset size:  66452\n",
      "val_dataset size:  17916\n",
      "test_dataset size:  21800\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4301\n",
      "1.0208293729462994\n",
      "1.0208293729463154\n",
      "1.0208293729462765\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.06771875323871206\n",
      "Jenson Shannon Distance for Train Test 0.06026124223007431\n",
      "Jenson Shannon Distance for Val Test 0.08971454087799793\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"davidson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17657, 4) (11468, 4) (2868, 4) (3321, 4)\n",
      "train_dataset size:  54671\n",
      "val_dataset size:  15225\n",
      "test_dataset size:  20701\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4529\n",
      "1.0265857914764973\n",
      "1.0265857914765046\n",
      "1.0265857914765284\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.1119078369639496\n",
      "Jenson Shannon Distance for Train Test 0.19135593616269655\n",
      "Jenson Shannon Distance for Val Test 0.21720498214426892\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"HASOC19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14100, 4) (10592, 4) (2648, 4) (860, 4)\n",
      "train_dataset size:  52132\n",
      "val_dataset size:  14702\n",
      "test_dataset size:  6646\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4470\n",
      "1.0316869788325564\n",
      "1.0316869788325191\n",
      "1.0316869788325425\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.08986306052867511\n",
      "Jenson Shannon Distance for Train Test 0.20220533295527315\n",
      "Jenson Shannon Distance for Val Test 0.2182595527729288\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"OLID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19600, 4) (13500, 4) (1500, 4) (4600, 4)\n",
      "train_dataset size:  62983\n",
      "val_dataset size:  9423\n",
      "test_dataset size:  25855\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4568\n",
      "1.0249734996936448\n",
      "1.0249734996936748\n",
      "1.0249734996936624\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.1480268255522918\n",
      "Jenson Shannon Distance for Train Test 0.18816951767161183\n",
      "Jenson Shannon Distance for Val Test 0.2588814208053837\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"HateEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21480, 4) (13747, 4) (3437, 4) (4296, 4)\n",
      "train_dataset size:  62303\n",
      "val_dataset size:  16648\n",
      "test_dataset size:  20385\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4266\n",
      "1.0218721646299675\n",
      "1.0218721646299782\n",
      "1.0218721646299747\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.07558284584057867\n",
      "Jenson Shannon Distance for Train Test 0.06984425027090946\n",
      "Jenson Shannon Distance for Val Test 0.10355789091427187\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"ImpHate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23748, 4) (15198, 4) (3800, 4) (4750, 4)\n",
      "train_dataset size:  98607\n",
      "val_dataset size:  25635\n",
      "test_dataset size:  31945\n",
      "train_set size:  2000\n",
      "val_set size:  2000\n",
      "test_set size:  2000\n",
      "union_set size:  4227\n",
      "1.0138828281821446\n",
      "1.0138828281821501\n",
      "1.0138828281821266\n",
      "\n",
      "\n",
      " ---- JS\n",
      "Jenson Shannon Distance for Train Val 0.06335808061195965\n",
      "Jenson Shannon Distance for Train Test 0.054082222278167356\n",
      "Jenson Shannon Distance for Val Test 0.08354065514387703\n"
     ]
    }
   ],
   "source": [
    "for_intra_dataset(\"ICDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = split_count(hate_dataset)\n",
    "# # print(' '.join(emoji for emoji in counter))\n",
    "# mc = Counter(counter)\n",
    "# print(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = split_count(offensive_dataset)\n",
    "# # print(' '.join(emoji for emoji in counter))\n",
    "# mc = Counter(counter)\n",
    "# print(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = split_count(provocative_dataset)\n",
    "# # print(' '.join(emoji for emoji in counter))\n",
    "# mc = Counter(counter)\n",
    "# print(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# counter = split_count(control_dataset)\n",
    "# # print(' '.join(emoji for emoji in counter))\n",
    "# mc = Counter(counter)\n",
    "# print(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_compare(BASE1,BASE2, multi=True,prune=True):\n",
    "#     df1, train_df, val_df, test_df = get_combined_df(BASE1)\n",
    "#     df2, train_df, val_df, test_df = get_combined_df(BASE2)\n",
    "#     column = 'clean_text'\n",
    "#     num_unq = len(df[\"label\"].unique())\n",
    "#     if num_unq >2 and multi:\n",
    "#         hate_dataset = preprocess_dataset(df[df['label'] <=1][column].values.tolist(),prune=prune)\n",
    "#         print(\"hate_dataset size: \", len(hate_dataset))\n",
    "#         control_dataset = preprocess_dataset(df[df['label'] > 1][column].values.tolist(),prune=prune)\n",
    "#         print(\"control_dataset size: \", len(control_dataset))\n",
    "#     else:\n",
    "#         hate_dataset = preprocess_dataset(df[df['label'] == 0][column].values.tolist(),prune=prune)\n",
    "#         print(\"hate_dataset size: \", len(hate_dataset))\n",
    "#         control_dataset = preprocess_dataset(df[df['label'] == 1][column].values.tolist(),prune=prune)\n",
    "#         print(\"control_dataset size: \", len(control_dataset))\n",
    "#     hate_set = set(hate_dataset)\n",
    "#     print(\"hate_set size: \", len(hate_set))\n",
    "#     control_set = set(control_dataset)\n",
    "#     print(\"control_set size: \", len(control_set))\n",
    "#     union_set = control_set.union(hate_set)\n",
    "#     print(\"union_set size: \", len(union_set))\n",
    "\n",
    "#     hate_counter = Counter(hate_dataset)\n",
    "#     control_counter = Counter(control_dataset)\n",
    "\n",
    "#     pdf_hate = []\n",
    "#     pdf_control = []\n",
    "#     pdf_hate_raw = []\n",
    "#     pdf_control_raw = []\n",
    "#     N = len(hate_dataset) + len(control_dataset)\n",
    "#     V = len(union_set)\n",
    "\n",
    "#     for word in union_set : \n",
    "#         if word in hate_set : \n",
    "#             pdf_hate.append((hate_counter[word]+1)/(len(hate_dataset) + len(hate_set)))\n",
    "#         else : \n",
    "#             pdf_hate.append(1/(N+V))\n",
    "#         pdf_hate_raw.append(hate_counter.get(word,0))\n",
    "#         if word in control_set : \n",
    "#             pdf_control.append((control_counter[word]+1)/(len(control_dataset) + len(control_set)))\n",
    "#         else : \n",
    "#             pdf_control.append(1/(N+V))\n",
    "#         pdf_control_raw.append(control_counter.get(word,0)) \n",
    "#     print(sum(pdf_hate))\n",
    "#     print(sum(pdf_control))\n",
    "\n",
    "#     print(\"\\n\\n ---- JS\")\n",
    "#     print(\"Jenson Shannon Distance for H N\", JSD(pdf_hate, pdf_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51367, 20) (41093, 20) (5137, 20) (5137, 20)\n"
     ]
    }
   ],
   "source": [
    " df, train_df, val_df, test_df = get_combined_df(\"HOPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>image</th>\n",
       "      <th>uid</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "      <th>n_likes</th>\n",
       "      <th>n_retweets</th>\n",
       "      <th>n_u_friends</th>\n",
       "      <th>n_u_followers</th>\n",
       "      <th>n_u_likes</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>n_hashtags</th>\n",
       "      <th>n_urls</th>\n",
       "      <th>n_emojis</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>817821482160046080</td>\n",
       "      <td>I haven't eaten at  #tacobell in years, but I ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17408077</td>\n",
       "      <td>3</td>\n",
       "      <td>Never Trump Campaign</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1028</td>\n",
       "      <td>373</td>\n",
       "      <td>26720</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I haven't eaten at taco bell in years, but I k...</td>\n",
       "      <td>Sat Jan 07 19:53:30 +0000 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1053729079525023745</td>\n",
       "      <td>@tnewtondunn @johnsweeneyroar really great pos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2522923753</td>\n",
       "      <td>1</td>\n",
       "      <td>Delhi Riots 2020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>1638</td>\n",
       "      <td>108541</td>\n",
       "      <td>2</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>$MENTION$ $MENTION$ really great posters aroun...</td>\n",
       "      <td>Sat Oct 20 19:26:04 +0000 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>456572669802995713</td>\n",
       "      <td>@_UmarKhalid @DMoreland95 @JordiCrookes Hahaaa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314662855</td>\n",
       "      <td>1</td>\n",
       "      <td>Delhi Riots 2020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>534</td>\n",
       "      <td>801</td>\n",
       "      <td>2650</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>$MENTION$ $MENTION$ $MENTION$ Hahaaa cba, I'm ...</td>\n",
       "      <td>Wed Apr 16 23:19:41 +0000 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1207127381330268160</td>\n",
       "      <td>#cash #Cashless 1 author encyclopedia site:Mod...</td>\n",
       "      <td>https://pbs.twimg.com/media/C2dtiaXXAAAgQJb.jpg</td>\n",
       "      <td>718130220746846208</td>\n",
       "      <td>3</td>\n",
       "      <td>Demonetization</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cash cashless 1 author encyclopedia site:Modi'...</td>\n",
       "      <td>Wed Dec 18 02:36:09 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>794159407294717952</td>\n",
       "      <td>God Save the Queen; women and children first! ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49362323</td>\n",
       "      <td>1</td>\n",
       "      <td>Brexit</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2564</td>\n",
       "      <td>3940</td>\n",
       "      <td>533</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>God Save the Queen; women and children first! ...</td>\n",
       "      <td>Thu Nov 03 12:48:52 +0000 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0   817821482160046080  I haven't eaten at  #tacobell in years, but I ...   \n",
       "1  1053729079525023745  @tnewtondunn @johnsweeneyroar really great pos...   \n",
       "2   456572669802995713  @_UmarKhalid @DMoreland95 @JordiCrookes Hahaaa...   \n",
       "3  1207127381330268160  #cash #Cashless 1 author encyclopedia site:Mod...   \n",
       "4   794159407294717952  God Save the Queen; women and children first! ...   \n",
       "\n",
       "                                             image                 uid  label  \\\n",
       "0                                              NaN            17408077      3   \n",
       "1                                              NaN          2522923753      1   \n",
       "2                                              NaN           314662855      1   \n",
       "3  https://pbs.twimg.com/media/C2dtiaXXAAAgQJb.jpg  718130220746846208      3   \n",
       "4                                              NaN            49362323      1   \n",
       "\n",
       "                  topic  n_likes  n_retweets  n_u_friends  n_u_followers  \\\n",
       "0  Never Trump Campaign        1           1         1028            373   \n",
       "1      Delhi Riots 2020        1           0          376           1638   \n",
       "2      Delhi Riots 2020        1           0          534            801   \n",
       "3        Demonetization        0           0            0              0   \n",
       "4                Brexit        2           2         2564           3940   \n",
       "\n",
       "   n_u_likes  n_upper  neg_score  pos_score  n_hashtags  n_urls  n_emojis  \\\n",
       "0      26720        2      0.000      0.000           3       1         0   \n",
       "1     108541        2      0.239      0.000           8       0         0   \n",
       "2       2650        0      0.092      0.164           0       0         1   \n",
       "3          0        0      0.000      0.000           3       2         0   \n",
       "4        533        0      0.272      0.000           1       1         1   \n",
       "\n",
       "   n_mentions                                         clean_text  \\\n",
       "0           0  I haven't eaten at taco bell in years, but I k...   \n",
       "1           2  $MENTION$ $MENTION$ really great posters aroun...   \n",
       "2           3  $MENTION$ $MENTION$ $MENTION$ Hahaaa cba, I'm ...   \n",
       "3           0  cash cashless 1 author encyclopedia site:Modi'...   \n",
       "4           0  God Save the Queen; women and children first! ...   \n",
       "\n",
       "                       created_at  \n",
       "0  Sat Jan 07 19:53:30 +0000 2017  \n",
       "1  Sat Oct 20 19:26:04 +0000 2018  \n",
       "2  Wed Apr 16 23:19:41 +0000 2014  \n",
       "3  Wed Dec 18 02:36:09 +0000 2019  \n",
       "4  Thu Nov 03 12:48:52 +0000 2016  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnlp_fig_id_list=[\n",
    "1233086926242054144,\n",
    "1232209842825089025,\n",
    "1158338062805610496,\n",
    "986964907877744641,\n",
    "1232733459099136001,\n",
    "1232591331379286017,\n",
    "1232770835645005824,\n",
    "1233235478737911809,\n",
    "1232615038491316226,\n",
    "680816856316403712\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "@@@@\n",
      "1233086926242054144\n",
      "scary! If libbies are still trying to defend peaceful #Tahir_hussain_terrorist you are an islamofacsist too #HinduLivesMatter #SaveHindusFromMuslims #SaveDelhiHindus https://t.co/0yVOG2Z8KX\n",
      "#HinduLivesMatter\n",
      "\n",
      "\n",
      "@@@@\n",
      "1232209842825089025\n",
      "and these animals are supported by many hindus also....congi and JDS politicians in karnataka...they are blaming police.....selfish pigs have their  own greed to fill...#DelhiRiots #DelhiBurning https://t.co/st6AQi6xg8\n",
      "Delhi Riots 2020\n",
      "\n",
      "\n",
      "@@@@\n",
      "1158338062805610496\n",
      "Great to see non-experts and laymen praise the situations with #Article370revoked and #Artical35A ! The season of uninformed, biased and majoritarian #WhatsApp messages is upon us, till 2024! This is totally going to work out like #Demonetization did. \n",
      "\n",
      "#Hallelujah !\n",
      "Demonetization\n",
      "\n",
      "\n",
      "@@@@\n",
      "986964907877744641\n",
      "While people reeled under #Demonetisation, BJP's income soared 81%, Amit Shah's son's income soared 16000%. No #BlackMoney recovered! #No2Modinomics which only works for the cronies! Yes to the #LeftAlternative of the #CPIM22PartyCongress! https://t.co/2dN6bIxKvt\n",
      "Demonetization\n",
      "\n",
      "\n",
      "@@@@\n",
      "1232733459099136001\n",
      "Full on Nazi mode activated by BJP. Murder, mayhem, midnight transfer of judges... dark days ahead\n",
      " #DelhiRiots2020\n",
      "Delhi Riots 2020\n",
      "\n",
      "\n",
      "@@@@\n",
      "1232591331379286017\n",
      "Delhi is facing the situation of fear and havoc because of #IslamicJihadis. The worst part is that the State-machinery seems to be ineffective in case of these Hinduphobic attacks in the face of anti-CAA protests.\n",
      "#DelhiRiots #Radical_Islamic_Terrorism\n",
      "\n",
      "https://t.co/5A4pbTEdX4 https://t.co/NKKRulPBJh\n",
      "Delhi Riots 2020\n",
      "\n",
      "\n",
      "@@@@\n",
      "1232770835645005824\n",
      "Impact of NRC/CAA:\n",
      "\n",
      "Number of Hindus lost their lives: 23\n",
      "\n",
      "Number of Muslims lost their Citizenship: ZERO.\n",
      "\n",
      "#DelhiRiots2020 #DelhiRiots\n",
      "Northeast Delhi Riots 2020\n",
      "\n",
      "\n",
      "@@@@\n",
      "1233235478737911809\n",
      "@_sabanaqvi This is what your quam has done, go through this 👇series of hate speeches your people have done since last 2-3 months. Illegal #ShaheenBaghProtest hate speeches are responsible for #DelhiRiots \n",
      "\n",
      "https://t.co/MfCwdL016v\n",
      "Delhi Riots 2020\n",
      "\n",
      "\n",
      "@@@@\n",
      "1232615038491316226\n",
      "This is #Modi's India, a cage for Minorities. Shame on you Terrorist, extremist, fundamentalist @narendramodi\n",
      "\n",
      ". You are the ugliest face of #Hindutva.\n",
      "\n",
      "#DelhiRiots #GenocideInDelhi #DelhiBurning #DelhiViolence #ArrestTerroristKapilMishra #TerroristModi https://t.co/tNnQbsp8jf\n",
      "Delhi Riots 2020\n",
      "\n",
      "\n",
      "@@@@\n",
      "680816856316403712\n",
      "Loved the Christmas day  speech!@royalfamilyinfo #IslamIsTheProblem #Brexit\n",
      "Brexit\n"
     ]
    }
   ],
   "source": [
    "for id_ in emnlp_fig_id_list:\n",
    "    print(\"\\n\\n@@@@\")\n",
    "    print(id_)\n",
    "    row = df[df[\"id\"]==id_].iloc[0]\n",
    "    assert row[\"id\"]==id_\n",
    "    assert row[\"label\"]==0\n",
    "#     print(row[\"id\"])\n",
    "    print(row[\"text\"])\n",
    "#     print(row[\"uid\"])\n",
    "    print(row[\"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                             1233086926242054144\n",
       "text             scary! If libbies are still trying to defend p...\n",
       "image                                                          NaN\n",
       "uid                                             864404114310135809\n",
       "label                                                            0\n",
       "topic                                            #HinduLivesMatter\n",
       "n_likes                                                          1\n",
       "n_retweets                                                       0\n",
       "n_u_friends                                                    133\n",
       "n_u_followers                                                   17\n",
       "n_u_likes                                                      112\n",
       "n_upper                                                          0\n",
       "neg_score                                                    0.137\n",
       "pos_score                                                    0.137\n",
       "n_hashtags                                                       4\n",
       "n_urls                                                           1\n",
       "n_emojis                                                         0\n",
       "n_mentions                                                       0\n",
       "clean_text       scary! If libbies are still trying to defend p...\n",
       "created_at                          Thu Feb 27 17:50:07 +0000 2020\n",
       "Name: 16708, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"id\"]==id_].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Never Trump Campaign'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"topic\"].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_info(topic):\n",
    "    print(\"\\n@@@@@@@@@@@@@\")\n",
    "    print(\"topic\",topic)\n",
    "    df_topic=df[df[\"topic\"]==topic]\n",
    "    assert len(df_topic[\"topic\"].unique())==1\n",
    "    unq_tids = len(df_topic)\n",
    "    unq_users = len(set(df_topic[\"uid\"]))\n",
    "    print(\"Unq ids and uids\", unq_tids,unq_users)\n",
    "#     print(\"Label wise ids\", df_topic.groupby(\"label\")[\"id\"].count())\n",
    "    uid0 = set(df_topic[df_topic[\"label\"]==0][\"uid\"])\n",
    "    uid1 = set(df_topic[df_topic[\"label\"]==1][\"uid\"])\n",
    "    uid2 = set(df_topic[df_topic[\"label\"]==2][\"uid\"])\n",
    "    uid3 = set(df_topic[df_topic[\"label\"]==3][\"uid\"])\n",
    "    \n",
    "    print(\"Label 0 unq ids and uids\",len(set(df_topic[df_topic[\"label\"]==0][\"id\"])),len(uid0))\n",
    "    print(\"Label 1 unq ids and uids\",len(set(df_topic[df_topic[\"label\"]==1][\"id\"])),len(uid1))\n",
    "    print(\"Label 2 unq ids and uids\",len(set(df_topic[df_topic[\"label\"]==2][\"id\"])),len(uid2))\n",
    "    print(\"Label 3 unq ids and uids\",len(set(df_topic[df_topic[\"label\"]==3][\"id\"])),len(uid3))\n",
    "    _0_ex = uid0.difference(uid1.union(uid2).union(uid3))\n",
    "    print(\"EX 0\",len(_0_ex))\n",
    "    _1_ex = uid1.difference(uid0.union(uid2).union(uid3))\n",
    "    print(\"EX 1\",len(_1_ex))\n",
    "    _2_ex = uid2.difference(uid1.union(uid0).union(uid3))\n",
    "    print(\"EX 2\",len(_2_ex))\n",
    "    _3_ex = uid3.difference(uid1.union(uid2).union(uid0))\n",
    "    print(\"EX 3\",len(_3_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic Never Trump Campaign\n",
      "Unq ids and uids 2257 1221\n",
      "Label 0 unq ids and uids 5 5\n",
      "Label 1 unq ids and uids 273 222\n",
      "Label 2 unq ids and uids 536 334\n",
      "Label 3 unq ids and uids 1443 889\n",
      "EX 0 1\n",
      "EX 1 130\n",
      "EX 2 185\n",
      "EX 3 707\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic Delhi Riots 2020\n",
      "Unq ids and uids 16858 8209\n",
      "Label 0 unq ids and uids 1503 1142\n",
      "Label 1 unq ids and uids 2071 1526\n",
      "Label 2 unq ids and uids 4776 3248\n",
      "Label 3 unq ids and uids 8508 4795\n",
      "EX 0 553\n",
      "EX 1 767\n",
      "EX 2 1871\n",
      "EX 3 3188\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic Demonetization\n",
      "Unq ids and uids 6578 3713\n",
      "Label 0 unq ids and uids 574 495\n",
      "Label 1 unq ids and uids 915 750\n",
      "Label 2 unq ids and uids 1446 1154\n",
      "Label 3 unq ids and uids 3643 2112\n",
      "EX 0 290\n",
      "EX 1 485\n",
      "EX 2 743\n",
      "EX 3 1601\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic Brexit\n",
      "Unq ids and uids 8127 4466\n",
      "Label 0 unq ids and uids 38 35\n",
      "Label 1 unq ids and uids 505 423\n",
      "Label 2 unq ids and uids 1041 805\n",
      "Label 3 unq ids and uids 6543 3654\n",
      "EX 0 17\n",
      "EX 1 247\n",
      "EX 2 515\n",
      "EX 3 3307\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic Umar Khalid JNU\n",
      "Unq ids and uids 5475 3776\n",
      "Label 0 unq ids and uids 70 63\n",
      "Label 1 unq ids and uids 2465 1967\n",
      "Label 2 unq ids and uids 18 17\n",
      "Label 3 unq ids and uids 2922 2028\n",
      "EX 0 50\n",
      "EX 1 1675\n",
      "EX 2 14\n",
      "EX 3 1743\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic Northeast Delhi Riots 2020\n",
      "Unq ids and uids 8773 6642\n",
      "Label 0 unq ids and uids 1182 1055\n",
      "Label 1 unq ids and uids 1622 1441\n",
      "Label 2 unq ids and uids 1605 1470\n",
      "Label 3 unq ids and uids 4364 3539\n",
      "EX 0 771\n",
      "EX 1 1097\n",
      "EX 2 1068\n",
      "EX 3 3007\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic #HinduLivesMatter\n",
      "Unq ids and uids 3299 1550\n",
      "Label 0 unq ids and uids 343 216\n",
      "Label 1 unq ids and uids 219 167\n",
      "Label 2 unq ids and uids 1134 552\n",
      "Label 3 unq ids and uids 1603 967\n",
      "EX 0 121\n",
      "EX 1 76\n",
      "EX 2 331\n",
      "EX 3 756\n",
      "\n",
      "@@@@@@@@@@@@@\n",
      "topic NOTA\n",
      "Unq ids and uids 0 0\n",
      "Label 0 unq ids and uids 0 0\n",
      "Label 1 unq ids and uids 0 0\n",
      "Label 2 unq ids and uids 0 0\n",
      "Label 3 unq ids and uids 0 0\n",
      "EX 0 0\n",
      "EX 1 0\n",
      "EX 2 0\n",
      "EX 3 0\n"
     ]
    }
   ],
   "source": [
    "for topic in df[\"topic\"].unique():\n",
    "    topic_info(topic)\n",
    "topic_info(\"NOTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unq ids and uids 51367 25796\n",
      "Label 0 unq ids and uids 3715 2925\n",
      "Label 1 unq ids and uids 8070 6161\n",
      "Label 2 unq ids and uids 10556 7088\n",
      "Label 3 unq ids and uids 29026 16186\n",
      "EX 0 1455\n",
      "EX 1 3724\n",
      "EX 2 3903\n",
      "EX 3 11928\n"
     ]
    }
   ],
   "source": [
    "unq_tids = len(df[\"id\"])\n",
    "unq_users = len(set(df[\"uid\"]))\n",
    "print(\"Unq ids and uids\", unq_tids,unq_users)\n",
    "uid0 = set(df[df[\"label\"]==0][\"uid\"])\n",
    "uid1 = set(df[df[\"label\"]==1][\"uid\"])\n",
    "uid2 = set(df[df[\"label\"]==2][\"uid\"])\n",
    "uid3 = set(df[df[\"label\"]==3][\"uid\"])\n",
    "print(\"Label 0 unq ids and uids\",len(set(df[df[\"label\"]==0][\"id\"])),len(uid0))\n",
    "print(\"Label 1 unq ids and uids\",len(set(df[df[\"label\"]==1][\"id\"])),len(uid1))\n",
    "print(\"Label 2 unq ids and uids\",len(set(df[df[\"label\"]==2][\"id\"])),len(uid2))\n",
    "print(\"Label 3 unq ids and uids\",len(set(df[df[\"label\"]==3][\"id\"])),len(uid3))\n",
    "_0_ex = uid0.difference(uid1.union(uid2).union(uid3))\n",
    "print(\"EX 0\",len(_0_ex))\n",
    "_1_ex = uid1.difference(uid0.union(uid2).union(uid3))\n",
    "print(\"EX 1\",len(_1_ex))\n",
    "_2_ex = uid2.difference(uid1.union(uid0).union(uid3))\n",
    "print(\"EX 2\",len(_2_ex))\n",
    "_3_ex = uid3.difference(uid1.union(uid2).union(uid0))\n",
    "print(\"EX 3\",len(_3_ex))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans_env",
   "language": "python",
   "name": "trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
